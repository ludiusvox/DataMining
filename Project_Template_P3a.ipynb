{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Template: Phase 3\n",
    "\n",
    "Below are some concrete steps that you can take while doing your analysis for phase 3. This guide isn't \"one size fit all\" so you will probably not do everything listed. But it still serves as a good \"pipeline\" for how to do data analysis.\n",
    "\n",
    "If you do engage in a step, you should clearly mention it in the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/aaronlinder/Documents/DataMining/week7/df2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0) Evaluation\n",
    "\n",
    "Now that you have selected your models and have trained/tuned them, it's time to see how they stack up. Some important questsion to ask:\n",
    "\n",
    "1. How did your models compare to each other\n",
    "2. In what metrics do they differ, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1) Comparing Models\n",
    "\n",
    "To compare your models, you can try things such as:\n",
    "\n",
    "1. Doing multiple random restarts of training/test splits (code below)\n",
    "2. Using cross-validation\n",
    "\n",
    "In your report, report back the following metrics:\n",
    "\n",
    "**Classification**\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1\n",
    "* AUC\n",
    "\n",
    "**Regression**\n",
    "* MSE\n",
    "* MAE\n",
    "* $R^2$\n",
    "\n",
    "**Sample Evaluation Code**: Here is some sample code for the evaluation procedure. **You do not need to use the sample code if you feel that it wouldn't work with your pipeline, but you can use it as inspiration**. It runs a set number of trials using different splits, and returns back a dataframe, where each row represents a single random evaluation. It has 3 columns.\n",
    "    \n",
    "Model: The name of the model being evaluated\n",
    "\n",
    "Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "\n",
    "Score: The score of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Evaluates a classification model\n",
    "\"\"\"\n",
    "def evaluate_classification(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    \n",
    "    acc = accuracy_score(y_test_ev,predictions)\n",
    "    \n",
    "    # Depending on the type of classification you are doing (e.g. multiclass vs binary)\n",
    "    # Make sure to change the \"average\" param depending on what you need\n",
    "    prec = precision_score(y_test_ev,predictions,average=\"weighted\",labels=np.unique(predictions))\n",
    "    recall = recall_score(y_test_ev,predictions,average=\"weighted\",labels=np.unique(predictions))\n",
    "    f1 = f1_score(y_test_ev,predictions,average=\"weighted\",labels=np.unique(predictions))\n",
    "    # Make sure to change/edit the `multi_class` of the ROC if you're doing multiclass\n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "    auc = roc_auc_score(y_test_ev,predictions, multi_class='ovr')\n",
    "    \n",
    "    return {\"acc\":acc,\"precision\":prec,\"recall\":recall,\"f1\":f1,\"auc\":auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "Evaluates regression using MAE,MSE, and R^2\n",
    "\"\"\"\n",
    "def evaluate_regression(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    mae = mean_absolute_error(y_test_ev,predictions)\n",
    "    mse = mean_squared_error(y_test_ev,predictions)\n",
    "    r2 = r2_score(y_test_ev,predictions)\n",
    "    return {\"mae\":mae,\"mse\":mse,\"r2\":r2}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains and evaluates a single model on a random train/test split\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def evaluate_random(model,X_train,y_train,X_test,y_test):\n",
    "    clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Switch this out with `evaluate_regression` if you're doing a regression problem\n",
    "    evals = evaluate_classification(model,X_test,y_test)\n",
    "    #evals = evaluate_regression(model,X_test,y_test)\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-fhgzhahu', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-igzy7xpl', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-w3kjy9r3', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-axjped2o', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-j3g99s7k', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-02uf9w24', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-a96icjkh', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-nlhmqkr8', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-88m3r__0', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-ztvtyt8t', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-xyujuefn', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-cauhcn0m', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-o8ofqkk_', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-kmee2xgu', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-byymb4pz', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/aaronlinder/Documents/DataMining/week10/dask-worker-space/worker-6cv1v7l1', purging\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: Your features\n",
    "    y: Your target\n",
    "    models: A list of the models that you are evaluating\n",
    "    n_trials (opt): The number of random trials\n",
    "    \n",
    "Output:\n",
    "    A dataframe with three colums and len(models)*n_trials*(number of evaluation metrics) rows.\n",
    "    Each row represents a single random evaluation.\n",
    "    \n",
    "    Model: The name of the model being evaluated\n",
    "    Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "    Score: The score of the evaluation\n",
    "\"\"\"\n",
    "def get_scores(X,y,models,n_trials=5):\n",
    "    \n",
    "    data = {\n",
    "        \"model\": ['logistic'],\n",
    "        \"evaluation\": ['precision'],\n",
    "        \"score\": [.0607],\n",
    "    }\n",
    "    \n",
    "    for n in range(n_trials):\n",
    "        for model in models:\n",
    "            # Put in special sampling methods\n",
    "            \n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "            # Put in feature scaling here\n",
    "            # MinMaxScaler()\n",
    "            try:\n",
    "                score = evaluate_random(model,X_train,y_train,X_test,y_test)\n",
    "                for key in score:\n",
    "                    data[\"model\"].append(str(model))\n",
    "                    data[\"evaluation\"].append(key)\n",
    "                    data[\"score\"].append(score[key])\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      model evaluation   score\n",
      "0  logistic  precision  0.0607\n"
     ]
    }
   ],
   "source": [
    "# Example of getting classification scores\n",
    "# (See \"Follow\" doc for how to do it with regression)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X,y = data.iloc[:,0:-1], data['Severity']\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3).compute()\n",
    "\n",
    "tally = get_scores(X,y,[neigh],5).head()\n",
    "print(tally)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a boxplot of the different random runs (see Follow document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tally' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5385/352384481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tally' is not defined"
     ]
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "tips = tally\n",
    "ax = sns.barplot(x=\"evaluation\", y=\"score\", data=tips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table with the average evaluation scores of each metric for each model\n",
    "**Bold** the best result for each.\n",
    "\n",
    "Here's an example for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | LinReg | SVR   | MLP   |\n",
    "|-----|--------|-------|-------|\n",
    "| **MAE** | 3.061  | 4.143 | **2.71**  |\n",
    "| **MSE** | 22.09  | 42.42 | **15.37** |\n",
    "| **$R^2$** | 0.684  | 0.394 | **0.780** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0) Technical Retrospective\n",
    "\n",
    "Now that you have your final model, go back and look at how your decisions impacted the results. This can take many forms, here are some ideas:\n",
    "\n",
    "* Which of your decisions were helpful? With your best model:\n",
    "    * Compare the results of the model with an without your feature selection\n",
    "    * Compare the results with and without feature engineering\n",
    "    * Compare if your sampling method made a difference\n",
    "    \n",
    "    \n",
    "* Why did your model do well?\n",
    "    * If your model is interpretable, discuss feature importance (e.g. decision tree splits, coefficients of linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0) Writeup \n",
    "\n",
    "Now it is time to reflect upon your work and tie up your report. The goal of this project was to get you familiar with doing a data science problem from scratch on a custom dataset. First, write some conclusions about your model. Then, consider how it could be used in practice. Finally, write about your experiences and what you learned from this project.\n",
    "\n",
    "Use the following questions as inspiration.\n",
    "\n",
    "1. How could we use this model in practice?\n",
    "2. Would you trust the model to make decisions?\n",
    "3. What are the limitations of the model?\n",
    "4. what are alternative approaches you could try in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset is too big for this state for my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes the model is fine, the limitations are computational power at this poitn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " using cloud computing to get faster answers\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
