{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 9: ANNs (SOLUTION)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Playground (Follow: Explore individually; Discuss as a Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, go to Tensorflow's [Neural Network Playground](https://playground.tensorflow.org/). This website is an interactive and exploratory visualization of how the features, number of layers, training time, etc, influence the classification boundries of an ANN. Right now, we'll only worry ourselves with *classification* problems.\n",
    "\n",
    "Play with the visualization, and then answer the following questions below.\n",
    "\n",
    "### Scenarios\n",
    "\n",
    "1. Using the default network topology, try training the network with the different activation functions (ReLU, Tanh, Sigmoid, Linear). What effect does the activation function have on the training time? What effect does the activation function have on the shape of the classification boundries?\n",
    "2. Take a look at [this setup](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.21855&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Train until the classification boundry converges. This is one of the rare cases where the nodes in an ANN can be (semi) interpreted. What do the nodes in the first hidden layer represent? What about the second hidden layer? How do you think the ANN uses these learned \"features\" to make a decision?\n",
    "\n",
    "### Exploration\n",
    "For each of the following questions:\n",
    "* Make a prediction before you begin exploring and testing.\n",
    "* Explain why you think this scenario has this property.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "3. Find a scenario where a simple model (fewer neurons) outperforms a complex model. (In regards to overfitting)\n",
    "4. Find a scenario where no hidden layers perform well.\n",
    "5. Find a scenario where a model with no hidden layers performs poorly no matter the features.\n",
    "6. Find a scenario where it takes a lot of training time to get a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Relue was better linear was bad\n",
    "2. create the linear boundaries\n",
    "3. less hidden layers was clearner\n",
    "4. the grid with the grid input\n",
    "5. the spiral\n",
    "6. the spiral plus 3 nuerons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training and Testing a Neural Network (Group)\n",
    "\n",
    "For this problem, you'll be looking at a reduced subset of the [Credit Card Fraud Data](https://www.kaggle.com/mlg-ulb/creditcardfraud), which contains transactions made by credit cards in September 2013 by European cardholders, including some fradulent transactions.\n",
    " \n",
    "There are two interesting properties about this dataset:\n",
    "\n",
    "1) **The data only contains dimensionality reduced data from a PCA transformation.** Sometimes, due to privacy concerns, all of the features (and even the names of the features used) cannot be known. Therefore, you'll be trying to train a model of data that has been reduced in dimensions with uninterpretable features.\n",
    "\n",
    "2) **The dataset is highly unbalanced.** The positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Knowing the data, what classification metrics (Precision, Recall, F1-Score) are most appropriate and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, **you have enough experience to do the entire model pipeline yourself**. That means *loading the data, creating splits, scaling the data, training and tuning the model, and evaluating the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data into a dataframe. Use `value_counts` to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      164032.0  0.013026  0.777210  0.168464 -0.782449  0.631586 -0.531628   \n",
       "1       63407.0 -0.227828  0.503434  0.960992  0.979314  0.074042  0.640817   \n",
       "2       75822.0  1.458861 -0.942226 -0.302423 -1.401064 -1.020394 -0.308819   \n",
       "3      168855.0  2.141957 -0.997336 -0.738212 -0.929019 -0.772330 -0.241391   \n",
       "4       67996.0  0.965124 -0.961507 -0.119976 -0.421448 -0.975116 -1.164778   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "85437   75618.0  1.173488  0.100792  0.490512  0.461596 -0.296377 -0.213165   \n",
       "85438  159000.0 -0.775981  0.144023 -1.142399 -1.241113  1.940358  3.912076   \n",
       "85439   79795.0 -0.146609  0.992946  1.524591  0.485774  0.349308 -0.815198   \n",
       "85440   87931.0 -2.948638  2.354849 -2.521201 -3.798905  1.866302  2.727695   \n",
       "85441   76381.0  1.233174 -0.784851  0.386784 -0.698559 -1.034018 -0.637028   \n",
       "\n",
       "             V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0      0.876275 -0.000646 -0.248065  ... -0.226443 -0.515073  0.029329   \n",
       "1      0.374438  0.014293  0.091550  ... -0.102313 -0.032916 -0.353239   \n",
       "2     -1.165356  0.024556 -1.870639  ... -0.081561  0.082309 -0.223705   \n",
       "3     -0.942758 -0.106791 -0.001484  ...  0.324429  0.973512  0.097843   \n",
       "4      0.272813 -0.443593 -1.284454  ... -0.655408 -1.954242  0.076510   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "85437 -0.165254  0.119221 -0.114199  ... -0.186027 -0.574283  0.161405   \n",
       "85438 -0.466107  1.360620  0.400697  ...  0.037078 -0.019575  0.241830   \n",
       "85439  1.076640 -0.395316 -0.491303  ...  0.052649  0.354089 -0.291198   \n",
       "85440 -0.471769  2.217537  0.580199  ... -0.332759 -1.047514  0.143326   \n",
       "85441 -0.502369 -0.188057 -0.749637  ...  0.027634 -0.234522 -0.059544   \n",
       "\n",
       "            V24       V25       V26       V27       V28  Amount  Class  \n",
       "0     -0.409008 -0.497966  0.147070  0.244097  0.082947    4.49      0  \n",
       "1     -0.947066  0.137538  0.735928 -0.026360 -0.006919   74.50      0  \n",
       "2     -0.656232  0.518888  0.010662  0.046806  0.040290   42.20      0  \n",
       "3      0.537377 -0.068501 -0.111042  0.006144 -0.037058   39.99      0  \n",
       "4      0.399212 -0.064425  0.595953 -0.112873  0.050798  239.00      0  \n",
       "...         ...       ...       ...       ...       ...     ...    ...  \n",
       "85437 -0.006140  0.091444  0.109235 -0.020922  0.003967    1.98      0  \n",
       "85438  0.682820 -1.635109 -0.770941  0.066006  0.137056   89.23      0  \n",
       "85439  0.402849  0.237383 -0.398467 -0.121139 -0.196195    3.94      0  \n",
       "85440  0.678869  0.319710  0.426309  0.496912  0.335822    1.00      0  \n",
       "85441 -0.109073  0.290326 -0.393074  0.001217  0.038588  113.00      0  \n",
       "\n",
       "[85442 rows x 31 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Partition the data into an X dataframe (features) and Y single-column dataframe (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164032.0</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>0.777210</td>\n",
       "      <td>0.168464</td>\n",
       "      <td>-0.782449</td>\n",
       "      <td>0.631586</td>\n",
       "      <td>-0.531628</td>\n",
       "      <td>0.876275</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.248065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035318</td>\n",
       "      <td>-0.226443</td>\n",
       "      <td>-0.515073</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>-0.409008</td>\n",
       "      <td>-0.497966</td>\n",
       "      <td>0.147070</td>\n",
       "      <td>0.244097</td>\n",
       "      <td>0.082947</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63407.0</td>\n",
       "      <td>-0.227828</td>\n",
       "      <td>0.503434</td>\n",
       "      <td>0.960992</td>\n",
       "      <td>0.979314</td>\n",
       "      <td>0.074042</td>\n",
       "      <td>0.640817</td>\n",
       "      <td>0.374438</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.091550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124125</td>\n",
       "      <td>-0.102313</td>\n",
       "      <td>-0.032916</td>\n",
       "      <td>-0.353239</td>\n",
       "      <td>-0.947066</td>\n",
       "      <td>0.137538</td>\n",
       "      <td>0.735928</td>\n",
       "      <td>-0.026360</td>\n",
       "      <td>-0.006919</td>\n",
       "      <td>74.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75822.0</td>\n",
       "      <td>1.458861</td>\n",
       "      <td>-0.942226</td>\n",
       "      <td>-0.302423</td>\n",
       "      <td>-1.401064</td>\n",
       "      <td>-1.020394</td>\n",
       "      <td>-0.308819</td>\n",
       "      <td>-1.165356</td>\n",
       "      <td>0.024556</td>\n",
       "      <td>-1.870639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211826</td>\n",
       "      <td>-0.081561</td>\n",
       "      <td>0.082309</td>\n",
       "      <td>-0.223705</td>\n",
       "      <td>-0.656232</td>\n",
       "      <td>0.518888</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.046806</td>\n",
       "      <td>0.040290</td>\n",
       "      <td>42.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168855.0</td>\n",
       "      <td>2.141957</td>\n",
       "      <td>-0.997336</td>\n",
       "      <td>-0.738212</td>\n",
       "      <td>-0.929019</td>\n",
       "      <td>-0.772330</td>\n",
       "      <td>-0.241391</td>\n",
       "      <td>-0.942758</td>\n",
       "      <td>-0.106791</td>\n",
       "      <td>-0.001484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124308</td>\n",
       "      <td>0.324429</td>\n",
       "      <td>0.973512</td>\n",
       "      <td>0.097843</td>\n",
       "      <td>0.537377</td>\n",
       "      <td>-0.068501</td>\n",
       "      <td>-0.111042</td>\n",
       "      <td>0.006144</td>\n",
       "      <td>-0.037058</td>\n",
       "      <td>39.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67996.0</td>\n",
       "      <td>0.965124</td>\n",
       "      <td>-0.961507</td>\n",
       "      <td>-0.119976</td>\n",
       "      <td>-0.421448</td>\n",
       "      <td>-0.975116</td>\n",
       "      <td>-1.164778</td>\n",
       "      <td>0.272813</td>\n",
       "      <td>-0.443593</td>\n",
       "      <td>-1.284454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>-0.655408</td>\n",
       "      <td>-1.954242</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.399212</td>\n",
       "      <td>-0.064425</td>\n",
       "      <td>0.595953</td>\n",
       "      <td>-0.112873</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>239.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85437</th>\n",
       "      <td>75618.0</td>\n",
       "      <td>1.173488</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>0.490512</td>\n",
       "      <td>0.461596</td>\n",
       "      <td>-0.296377</td>\n",
       "      <td>-0.213165</td>\n",
       "      <td>-0.165254</td>\n",
       "      <td>0.119221</td>\n",
       "      <td>-0.114199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157534</td>\n",
       "      <td>-0.186027</td>\n",
       "      <td>-0.574283</td>\n",
       "      <td>0.161405</td>\n",
       "      <td>-0.006140</td>\n",
       "      <td>0.091444</td>\n",
       "      <td>0.109235</td>\n",
       "      <td>-0.020922</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85438</th>\n",
       "      <td>159000.0</td>\n",
       "      <td>-0.775981</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>-1.142399</td>\n",
       "      <td>-1.241113</td>\n",
       "      <td>1.940358</td>\n",
       "      <td>3.912076</td>\n",
       "      <td>-0.466107</td>\n",
       "      <td>1.360620</td>\n",
       "      <td>0.400697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295730</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>-0.019575</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.682820</td>\n",
       "      <td>-1.635109</td>\n",
       "      <td>-0.770941</td>\n",
       "      <td>0.066006</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>89.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85439</th>\n",
       "      <td>79795.0</td>\n",
       "      <td>-0.146609</td>\n",
       "      <td>0.992946</td>\n",
       "      <td>1.524591</td>\n",
       "      <td>0.485774</td>\n",
       "      <td>0.349308</td>\n",
       "      <td>-0.815198</td>\n",
       "      <td>1.076640</td>\n",
       "      <td>-0.395316</td>\n",
       "      <td>-0.491303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.052649</td>\n",
       "      <td>0.354089</td>\n",
       "      <td>-0.291198</td>\n",
       "      <td>0.402849</td>\n",
       "      <td>0.237383</td>\n",
       "      <td>-0.398467</td>\n",
       "      <td>-0.121139</td>\n",
       "      <td>-0.196195</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85440</th>\n",
       "      <td>87931.0</td>\n",
       "      <td>-2.948638</td>\n",
       "      <td>2.354849</td>\n",
       "      <td>-2.521201</td>\n",
       "      <td>-3.798905</td>\n",
       "      <td>1.866302</td>\n",
       "      <td>2.727695</td>\n",
       "      <td>-0.471769</td>\n",
       "      <td>2.217537</td>\n",
       "      <td>0.580199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417396</td>\n",
       "      <td>-0.332759</td>\n",
       "      <td>-1.047514</td>\n",
       "      <td>0.143326</td>\n",
       "      <td>0.678869</td>\n",
       "      <td>0.319710</td>\n",
       "      <td>0.426309</td>\n",
       "      <td>0.496912</td>\n",
       "      <td>0.335822</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85441</th>\n",
       "      <td>76381.0</td>\n",
       "      <td>1.233174</td>\n",
       "      <td>-0.784851</td>\n",
       "      <td>0.386784</td>\n",
       "      <td>-0.698559</td>\n",
       "      <td>-1.034018</td>\n",
       "      <td>-0.637028</td>\n",
       "      <td>-0.502369</td>\n",
       "      <td>-0.188057</td>\n",
       "      <td>-0.749637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.027634</td>\n",
       "      <td>-0.234522</td>\n",
       "      <td>-0.059544</td>\n",
       "      <td>-0.109073</td>\n",
       "      <td>0.290326</td>\n",
       "      <td>-0.393074</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>113.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85442 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      164032.0  0.013026  0.777210  0.168464 -0.782449  0.631586 -0.531628   \n",
       "1       63407.0 -0.227828  0.503434  0.960992  0.979314  0.074042  0.640817   \n",
       "2       75822.0  1.458861 -0.942226 -0.302423 -1.401064 -1.020394 -0.308819   \n",
       "3      168855.0  2.141957 -0.997336 -0.738212 -0.929019 -0.772330 -0.241391   \n",
       "4       67996.0  0.965124 -0.961507 -0.119976 -0.421448 -0.975116 -1.164778   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "85437   75618.0  1.173488  0.100792  0.490512  0.461596 -0.296377 -0.213165   \n",
       "85438  159000.0 -0.775981  0.144023 -1.142399 -1.241113  1.940358  3.912076   \n",
       "85439   79795.0 -0.146609  0.992946  1.524591  0.485774  0.349308 -0.815198   \n",
       "85440   87931.0 -2.948638  2.354849 -2.521201 -3.798905  1.866302  2.727695   \n",
       "85441   76381.0  1.233174 -0.784851  0.386784 -0.698559 -1.034018 -0.637028   \n",
       "\n",
       "             V7        V8        V9  ...       V20       V21       V22  \\\n",
       "0      0.876275 -0.000646 -0.248065  ... -0.035318 -0.226443 -0.515073   \n",
       "1      0.374438  0.014293  0.091550  ...  0.124125 -0.102313 -0.032916   \n",
       "2     -1.165356  0.024556 -1.870639  ... -0.211826 -0.081561  0.082309   \n",
       "3     -0.942758 -0.106791 -0.001484  ...  0.124308  0.324429  0.973512   \n",
       "4      0.272813 -0.443593 -1.284454  ...  0.049000 -0.655408 -1.954242   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "85437 -0.165254  0.119221 -0.114199  ... -0.157534 -0.186027 -0.574283   \n",
       "85438 -0.466107  1.360620  0.400697  ... -0.295730  0.037078 -0.019575   \n",
       "85439  1.076640 -0.395316 -0.491303  ...  0.007155  0.052649  0.354089   \n",
       "85440 -0.471769  2.217537  0.580199  ...  0.417396 -0.332759 -1.047514   \n",
       "85441 -0.502369 -0.188057 -0.749637  ...  0.337732  0.027634 -0.234522   \n",
       "\n",
       "            V23       V24       V25       V26       V27       V28  Amount  \n",
       "0      0.029329 -0.409008 -0.497966  0.147070  0.244097  0.082947    4.49  \n",
       "1     -0.353239 -0.947066  0.137538  0.735928 -0.026360 -0.006919   74.50  \n",
       "2     -0.223705 -0.656232  0.518888  0.010662  0.046806  0.040290   42.20  \n",
       "3      0.097843  0.537377 -0.068501 -0.111042  0.006144 -0.037058   39.99  \n",
       "4      0.076510  0.399212 -0.064425  0.595953 -0.112873  0.050798  239.00  \n",
       "...         ...       ...       ...       ...       ...       ...     ...  \n",
       "85437  0.161405 -0.006140  0.091444  0.109235 -0.020922  0.003967    1.98  \n",
       "85438  0.241830  0.682820 -1.635109 -0.770941  0.066006  0.137056   89.23  \n",
       "85439 -0.291198  0.402849  0.237383 -0.398467 -0.121139 -0.196195    3.94  \n",
       "85440  0.143326  0.678869  0.319710  0.426309  0.496912  0.335822    1.00  \n",
       "85441 -0.059544 -0.109073  0.290326 -0.393074  0.001217  0.038588  113.00  \n",
       "\n",
       "[85442 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "85437    0\n",
       "85438    0\n",
       "85439    0\n",
       "85440    0\n",
       "85441    0\n",
       "Name: Class, Length: 85442, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = df['Class']\n",
    "X = df.drop(columns=['Class'])\n",
    "\n",
    "display(X)\n",
    "display(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create your train/test split. Use the provided random_state.\n",
    "\n",
    "**Note**: You should use a `train_size` of 0.3, or 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use a [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to standardize the data. \n",
    "\n",
    "Fit the scaler only the the training X features, and then apply it to both training and test X features. We do this because in practice, we wouldn't be able to see data in the test X, so it shouldn't affect feature transformation. We therefore only use X_train for feature transformation.\n",
    "\n",
    "**Note**: Even though most of the features are already transformed using PCA (which would not require additional standardize), there is one other feature (time) that is not, so we should scale as a best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72784000e+05, 2.41826698e+00, 2.20577290e+01, 4.04046500e+00,\n",
       "       1.64912172e+01, 3.48016659e+01, 7.33016255e+01, 1.20589494e+02,\n",
       "       2.00072084e+01, 9.09908155e+00, 1.32494330e+01, 1.04468468e+01,\n",
       "       4.31807084e+00, 7.12688296e+00, 7.17616083e+00, 8.87774160e+00,\n",
       "       1.73151115e+01, 9.25352625e+00, 4.29564823e+00, 5.50174721e+00,\n",
       "       2.62373908e+01, 2.18997238e+01, 8.31627544e+00, 2.08033441e+01,\n",
       "       4.58454914e+00, 5.82615903e+00, 3.41563624e+00, 3.16121981e+01,\n",
       "       3.38478078e+01, 2.56911600e+04])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(X_train)\n",
    "mms.transform(X_train)\n",
    "mms.data_max_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 30 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23340/1447023231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 30 into shape (28,28)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Train an MLP with default hyperparameters.\n",
    "\n",
    "For the following, you'll be using sklearn's built in Multi-layer Perceptron classifier [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Use the default hyperparams aside from `max_iter`. `max_iter` is how many iterations of training the ANN goes though until it manually stops. The default `max_iter=200` is too long for our data currently. \n",
    "\n",
    "**Use random_state as the random_states and max_iter=20**. The detault parameters will use a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977762172284644"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=20)\n",
    "mlp.fit(X, Y)\n",
    "y_pred = mlp.predict(X_test)\n",
    "mlp.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 30 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23340/1447023231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 30 into shape (28,28)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## Ignore playing around with plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Evaluate the model on the test dataset using a confusion matrix and a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59613,    88],\n",
       "       [   45,    64]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59701\n",
      "           1       0.42      0.59      0.49       109\n",
      "\n",
      "    accuracy                           1.00     59810\n",
      "   macro avg       0.71      0.79      0.74     59810\n",
      "weighted avg       1.00      1.00      1.00     59810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, your model should have an accuracy of almost 100%. Use `classification_report` to explain what you think happened. Is the model performing well? If not, is it overfitting or underfitting? Remember that the classes in the problem are very imbalanced, but out main goal is to detect fraud (class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `classification_report` outputs Precision, Recall and F1 for both classes. Remember that how we calculate these metrics depends on which class we treat as the positive class. If we say Class 0 is the positive class, a FP means incorrectly predicting Class 0, but for Class 1 a FP is incorrectly predicting Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averages are in line with not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hyperparameters (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparams**:\n",
    "\n",
    "ANNs have *a lot* of hyperparams. This can include simple things such as the number of layers and nodes, up to tuning the learning rate and the gradient descent algorithm used. \n",
    "\n",
    "Unfortunately, there is no tried an true method for selecting hyperparams for a neural network. It requires a lot of experimentation and intution through experience. (In fact, one of the most successful methods in training neural networks is *Graduate Student Descent*, where you simply give the laborious process of tuning to a graduate student while you go and do more research!)\n",
    "\n",
    "For now, the paramaters that you should explore are:\n",
    "\n",
    "* `activation`: The activation function of the the ANN. Defaults to ReLU.\n",
    "* `max_iter`: The ANN will train iterations until either the loss stops improving by a specified threshold, or `max_iters` is reached. Warning: the more you increase this, the more the training time will take! Patience is a virtue.\n",
    "* `hidden_layer_sizes`: A tuple representing the structure of the hidden layers. For example, giving the tuple `(100,50)` means that there's two hidden layers: the first being of size 100, and the second being of size 50. The tuple (100,) would mean a single hidden layer of size 100.\n",
    "\n",
    "**Try different permutations of these hyperprams and see how it affects the classification scores of your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. What criteria did you use to determine which model hyperparameters performed \"best\"? Why? Justify your answer with respect to the problem: fraud detection.\n",
    "2. What hyperparameters performed best. Why do you think they performed best?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
