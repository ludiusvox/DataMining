{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop 6**\n",
    "\n",
    "In this workshop, you'll be working with KNN and AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Problem 1\n",
    "The helper function below will help you with the first Theory problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00,  5.02,  5.30,  3.77, 12.43,  3.85,  6.52,  3.24,  6.50,  4.99],\n",
       "       [ 5.02,  0.00, 10.19,  7.32,  8.00,  1.18, 10.72,  7.90, 11.43, 10.02],\n",
       "       [ 5.30, 10.19,  0.00,  4.07, 16.82,  9.05,  2.88,  2.38,  1.25,  1.70],\n",
       "       [ 3.77,  7.32,  4.07,  0.00, 12.98,  6.37,  3.44,  2.08,  5.21,  5.04],\n",
       "       [12.43,  8.00, 16.82, 12.98,  0.00,  9.04, 16.21, 14.46, 18.06, 17.17],\n",
       "       [ 3.85,  1.18,  9.05,  6.37,  9.04,  0.00,  9.71,  6.79, 10.28,  8.84],\n",
       "       [ 6.52, 10.72,  2.88,  3.44, 16.21,  9.71,  0.00,  3.36,  3.28,  4.57],\n",
       "       [ 3.24,  7.90,  2.38,  2.08, 14.46,  6.79,  3.36,  0.00,  3.63,  3.00],\n",
       "       [ 6.50, 11.43,  1.25,  5.21, 18.06, 10.28,  3.28,  3.63,  0.00,  2.13],\n",
       "       [ 4.99, 10.02,  1.70,  5.04, 17.17,  8.84,  4.57,  3.00,  2.13,  0.00]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [-3.44, 1 ],\n",
    "    [-6.48, 5],\n",
    "    [0.93, -2],\n",
    "    [0.2, 2],\n",
    "    [-6.69, 13 ],\n",
    "    [-5.85, 4],\n",
    "    [3.0, 0],\n",
    "    [-0.36, 0],\n",
    "    [1.68, -3],\n",
    "    [-0.45, -3]\n",
    "    \n",
    "])\n",
    "np.set_printoptions(precision=2, linewidth=100, floatmode='fixed')\n",
    "metrics.pairwise_distances(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Loading Data & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# set a seed for reproducibility\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Twitter Sentiment Analysis using KNN\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is the study of how we can systematically identify and quantify sentiment of a given segment of text. In this problem, you will be using the [Sentiment140](http://help.sentiment140.com/for-students/) dataset to build a classifier that will rate the sentiment of a string of text on a scale from 0 to 4.\n",
    "\n",
    "**WARNING:** This is *real* data from *real* people from Twitter. That means you might (and probably will) see some unscrupulous language when browsing the dataset. Don't browse with kids around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data, and what attributes we have.\n",
    "\n",
    "* **polarity**: The assumed polarity of the tweet. For this subset, we're only considering positive and negative tweets, no neutrality.\n",
    "* **id**: The tweet ID.\n",
    "* **date**: The date the tweet was posted.\n",
    "* **query**: The search term used in order to find tweets of a certain topic.\n",
    "* **text**: The actual text of the tweet.\n",
    "\n",
    "For now, we only consider the **polarity** and **text** attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1468251446</td>\n",
       "      <td>Tue Apr 07 00:33:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>derek319</td>\n",
       "      <td>www.killerjo.net try not to focus on science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2052426181</td>\n",
       "      <td>Sat Jun 06 00:41:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DJSunshine1980</td>\n",
       "      <td>After Rain following often the Sun, so don't b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1692413347</td>\n",
       "      <td>Sun May 03 19:56:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MicheleBlueston</td>\n",
       "      <td>@Sixtiesguy Hey it's sexy to be unemployed...a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2176577445</td>\n",
       "      <td>Mon Jun 15 04:14:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>is_selene</td>\n",
       "      <td>NVM. Thank god for auto recovery. I didn't los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1771234509</td>\n",
       "      <td>Mon May 11 23:38:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>STARj0NES314</td>\n",
       "      <td>@IamSpectacular damn like dat ?  hahaha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                          date     query  \\\n",
       "0         4  1468251446  Tue Apr 07 00:33:05 PDT 2009  NO_QUERY   \n",
       "1         4  2052426181  Sat Jun 06 00:41:22 PDT 2009  NO_QUERY   \n",
       "2         4  1692413347  Sun May 03 19:56:39 PDT 2009  NO_QUERY   \n",
       "3         4  2176577445  Mon Jun 15 04:14:20 PDT 2009  NO_QUERY   \n",
       "4         4  1771234509  Mon May 11 23:38:42 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0         derek319      www.killerjo.net try not to focus on science   \n",
       "1   DJSunshine1980  After Rain following often the Sun, so don't b...  \n",
       "2  MicheleBlueston  @Sixtiesguy Hey it's sexy to be unemployed...a...  \n",
       "3        is_selene  NVM. Thank god for auto recovery. I didn't los...  \n",
       "4     STARj0NES314            @IamSpectacular damn like dat ?  hahaha  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    345223\n",
       "0    344777\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our disribution of polarity\n",
    "# 4 means postive sentiment\n",
    "# 0 means negative sentiment\n",
    "train_data.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Brief Introduction to tf-idf (Follow)\n",
    "In information retreival [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (*term frequency â€“ inverse document frequency*) is a metric that represents how \"important\" a word is to a corpus of text. While we won't go into detail about how it works, essentially all you need to know is that it balances two metrics.\n",
    "\n",
    "**Term Frequency**: Is exactly what one would expect, it is the the frequency at which a word is present in a corpus of text. A word with a higher term-frequency score appears much more in the corpus compared to one with a low term frequency.\n",
    "\n",
    "**Inverse Document Frequency**: If we only used term frequency, common words like \"the\" or \"and\" would have a high score, even though they don't give us that much information since they are present in every document. *Inverse Document Frequency* is a metric of how much \"information\" a word provides, and if a word is common or rare across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf vectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how important word `m` is to document `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000 0.4698 0.5803 0.3841 0.0000 0.0000 0.3841 0.0000 0.3841]\n",
      " [0.0000 0.6876 0.0000 0.2811 0.0000 0.5386 0.2811 0.0000 0.2811]\n",
      " [0.5118 0.0000 0.0000 0.2671 0.5118 0.0000 0.2671 0.5118 0.2671]\n",
      " [0.0000 0.4698 0.5803 0.3841 0.0000 0.0000 0.3841 0.0000 0.3841]]\n"
     ]
    }
   ],
   "source": [
    "# Printing out the tf-idf matrix\n",
    "np.set_printoptions(precision=4)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that if we try and print X directly, we get an overview saying that X is a \"sparse matrix\".\n",
    "# In very large corpi with many unique words, a lot of row entries are going to consist of majority zeros\n",
    "# Thus numpy saves theses in a special compressed sparse format\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next let's see what word each column corresponds to:\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tf-idf vectors for two different documents.\n",
    "\n",
    "**Note**: `dic(zip(A, B))` in pyton makes a dictionary out of a list of keys (A) and values (B). This just makes it easier to view each term with it's corresponding TFIDF value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first document.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0.0,\n",
       " 'document': 0.46979138557992045,\n",
       " 'first': 0.5802858236844359,\n",
       " 'is': 0.38408524091481483,\n",
       " 'one': 0.0,\n",
       " 'second': 0.0,\n",
       " 'the': 0.38408524091481483,\n",
       " 'third': 0.0,\n",
       " 'this': 0.38408524091481483}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "dict(zip(vectorizer.get_feature_names(),X.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is the second document.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0.0,\n",
       " 'document': 0.6876235979836938,\n",
       " 'first': 0.0,\n",
       " 'is': 0.281088674033753,\n",
       " 'one': 0.0,\n",
       " 'second': 0.5386476208856763,\n",
       " 'the': 0.281088674033753,\n",
       " 'third': 0.0,\n",
       " 'this': 0.281088674033753}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus[1])\n",
    "dict(zip(vectorizer.get_feature_names(),X.toarray()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the tf-idf vectors for both of these sentences and answer the following questions:\n",
    "1. Why is the value for the term \"is\" higher in document1 than document2?\n",
    "2. Why is the value for the term \"document\" higher in document2 than document1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Model Design Discussion (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're aware of what tf-idf scores are, **discuss with your group on how you could use tf-idf to build a KNN classifier**:\n",
    "1. If the data objects (rows) each represent a tweet, what would the features (columns) of the dataset be?\n",
    "2. How would you represent a single tweet as a vector of numbers (values for each of the features)? What would it mean to have a value of 0 for a given feature? What about 0.4?\n",
    "3. What would it mean for two sentences to be \"nearest neighbors\"?\n",
    "4. Would you expect two near neighbors to have similar sentiment? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text is the best feature column because it's the content or text of the tweet.\n",
    "1. Then its the frequency of the word\n",
    "1. They have words in common, so they are near each other in the vector space.\n",
    "1. No, just similar words, computer dosent know tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) tf-idf on Twitter (Follow)\n",
    "\n",
    "Now, before we build a classifier, let's just try and see what the nearest neighbors of a specified message are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our text\n",
    "corpus = train_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our transform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tfidf_matrix = tf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<690000x387729 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8185910 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the size of our matrix\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit the nearest neighbors tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your KNN model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run custom sentences and see what sentences in the corpus are \"closest\" to what we put in. Try a few and see what shows up! In addition to this, you can change the `n_neighbors` param and get more queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617716</th>\n",
       "      <td>0</td>\n",
       "      <td>2265303137</td>\n",
       "      <td>Sun Jun 21 06:16:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>beesjebinda</td>\n",
       "      <td>Doing our taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124623</th>\n",
       "      <td>0</td>\n",
       "      <td>2194131231</td>\n",
       "      <td>Tue Jun 16 09:23:08 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HeyKirby</td>\n",
       "      <td>I don't wanna pay taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100946</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484942</th>\n",
       "      <td>0</td>\n",
       "      <td>1551900745</td>\n",
       "      <td>Sat Apr 18 10:15:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Slushie76</td>\n",
       "      <td>working on taxes and marks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550210</th>\n",
       "      <td>0</td>\n",
       "      <td>2215612962</td>\n",
       "      <td>Wed Jun 17 18:22:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nathanbraceros</td>\n",
       "      <td>Thinking about taxes. How depressing!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity          id                          date     query  \\\n",
       "617716         0  2265303137  Sun Jun 21 06:16:29 PDT 2009  NO_QUERY   \n",
       "124623         0  2194131231  Tue Jun 16 09:23:08 PDT 2009  NO_QUERY   \n",
       "100946         0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY   \n",
       "484942         0  1551900745  Sat Apr 18 10:15:56 PDT 2009  NO_QUERY   \n",
       "550210         0  2215612962  Wed Jun 17 18:22:36 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                    text  \n",
       "617716     beesjebinda                        Doing our taxes   \n",
       "124623        HeyKirby                I don't wanna pay taxes   \n",
       "100946        armotley                    about to file taxes   \n",
       "484942       Slushie76          working on taxes and marks...   \n",
       "550210  nathanbraceros  Thinking about taxes. How depressing!   "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a tweet!\n",
    "test_docs = [\"I sure love paying my taxes!\"]\n",
    "test_docs = tf.transform(test_docs)\n",
    "distances, indicies = nbrs.kneighbors(test_docs)\n",
    "train_data.iloc[indicies[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505723</th>\n",
       "      <td>0</td>\n",
       "      <td>1985240146</td>\n",
       "      <td>Sun May 31 16:42:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cutekiwi</td>\n",
       "      <td>Wishes it was dnd week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435832</th>\n",
       "      <td>0</td>\n",
       "      <td>1963236303</td>\n",
       "      <td>Fri May 29 12:25:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kkelss</td>\n",
       "      <td>Ugh! I wanna play dnd but i know i'm going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59813</th>\n",
       "      <td>0</td>\n",
       "      <td>2209772354</td>\n",
       "      <td>Wed Jun 17 10:46:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kristin428</td>\n",
       "      <td>Horrible migraine today. Might have to cancel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672042</th>\n",
       "      <td>0</td>\n",
       "      <td>2175603421</td>\n",
       "      <td>Mon Jun 15 01:27:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ruth_ie</td>\n",
       "      <td>DND I have a migrain...i think my head might e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64275</th>\n",
       "      <td>0</td>\n",
       "      <td>2044129048</td>\n",
       "      <td>Fri Jun 05 08:54:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>goctopus</td>\n",
       "      <td>WHY WON'T IT LET ME PLAY?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity          id                          date     query  \\\n",
       "505723         0  1985240146  Sun May 31 16:42:51 PDT 2009  NO_QUERY   \n",
       "435832         0  1963236303  Fri May 29 12:25:00 PDT 2009  NO_QUERY   \n",
       "59813          0  2209772354  Wed Jun 17 10:46:44 PDT 2009  NO_QUERY   \n",
       "672042         0  2175603421  Mon Jun 15 01:27:33 PDT 2009  NO_QUERY   \n",
       "64275          0  2044129048  Fri Jun 05 08:54:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "505723    cutekiwi                            Wishes it was dnd week   \n",
       "435832      kkelss  Ugh! I wanna play dnd but i know i'm going to ...  \n",
       "59813   Kristin428  Horrible migraine today. Might have to cancel ...  \n",
       "672042     ruth_ie  DND I have a migrain...i think my head might e...  \n",
       "64275     goctopus                         WHY WON'T IT LET ME PLAY?   "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\"let's play DnD \"]\n",
    "test_docs = tf.transform(test_docs)\n",
    "distances, indicies = nbrs.kneighbors(test_docs)\n",
    "train_data.iloc[indicies[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329867</th>\n",
       "      <td>4</td>\n",
       "      <td>1677327631</td>\n",
       "      <td>Sat May 02 01:30:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kinia1245</td>\n",
       "      <td>Clean my house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169334</th>\n",
       "      <td>4</td>\n",
       "      <td>1827698955</td>\n",
       "      <td>Sun May 17 11:15:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>idyllicwater</td>\n",
       "      <td>Clean house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641029</th>\n",
       "      <td>0</td>\n",
       "      <td>1994424844</td>\n",
       "      <td>Mon Jun 01 11:54:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luckypiano1996</td>\n",
       "      <td>have to clean the house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237226</th>\n",
       "      <td>0</td>\n",
       "      <td>2194627321</td>\n",
       "      <td>Tue Jun 16 10:02:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>burnett4923</td>\n",
       "      <td>Really need to clean my house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610328</th>\n",
       "      <td>0</td>\n",
       "      <td>2054604053</td>\n",
       "      <td>Sat Jun 06 07:42:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaylaspiveyy</td>\n",
       "      <td>gotta clean the house.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity          id                          date     query  \\\n",
       "329867         4  1677327631  Sat May 02 01:30:56 PDT 2009  NO_QUERY   \n",
       "169334         4  1827698955  Sun May 17 11:15:37 PDT 2009  NO_QUERY   \n",
       "641029         0  1994424844  Mon Jun 01 11:54:16 PDT 2009  NO_QUERY   \n",
       "237226         0  2194627321  Tue Jun 16 10:02:53 PDT 2009  NO_QUERY   \n",
       "610328         0  2054604053  Sat Jun 06 07:42:35 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                            text  \n",
       "329867       kinia1245                 Clean my house   \n",
       "169334    idyllicwater                    Clean house   \n",
       "641029  luckypiano1996        have to clean the house   \n",
       "237226     burnett4923  Really need to clean my house   \n",
       "610328    kaylaspiveyy         gotta clean the house.   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\"Keeping my house clean\"]\n",
    "test_docs = tf.transform(test_docs)\n",
    "distances, indicies = nbrs.kneighbors(test_docs)\n",
    "train_data.iloc[indicies[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now discuss together:\n",
    "1. Try manually classifying the tweet \"Wow, this is so cool!\" What are the classes of the neighbors? How would a 5-NN classifier classify that tweet?\n",
    "2. Can you think of a tweet that might fool this classifier? For example, how would it do with sarcasm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Building and Evaluating the Classifier (Group)\n",
    "\n",
    "Now, your goal is to use the [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) in order to build an actual classifier for the sentiment dataset. Build the classifier with a `k=5`, and then evaluate it on the test data. Print the confusion matrix and a classification report, and interpret the evaluation metrics. Remember, a class label of `0` means a negative sentiment, and a class label of `1` means a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./data/test_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  id                          date    query      user  \\\n",
       "0         4   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1         4   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2         4   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3         4   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4         4   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your X features (text) and y labels (polarity) for the test set\n",
    "# Transform the X features into TFIDF vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_data_fraction = 0.2\n",
    "y = train_data[\"polarity\"]\n",
    "corpus = train_data[\"text\"]\n",
    "tf = TfidfVectorizer()\n",
    "tfidf_matrix = tf.fit_transform(corpus)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tfidf_matrix, y, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train your KNN model with k=5\n",
    "nbrs = KNeighborsClassifier(n_neighbors=5)\n",
    "nbrs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49971, 18997],\n",
       "       [22560, 46472]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a confusion matrix of the predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predict = nbrs.predict(X_test)\n",
    "confusion_matrix(Y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71     68968\n",
      "           4       0.71      0.67      0.69     69032\n",
      "\n",
      "    accuracy                           0.70    138000\n",
      "   macro avg       0.70      0.70      0.70    138000\n",
      "weighted avg       0.70      0.70      0.70    138000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the classification_report to report how well your model did\n",
    "print(classification_report(Y_test,predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, discuss with your group about your results.\n",
    "1. How well do you think the classifier performed overall?\n",
    "2. Can you find some examples of test instances that were not accurately classified? Based on their nearest neighbots in the training dataset, why do you think they were hard to classify?\n",
    "3. We represented each tweet using TF-IDF, representing the frequency of individual words. What other types of features might be important for predicting the sentiment of a tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       177\n",
      "           4       0.73      0.71      0.72       182\n",
      "\n",
      "    accuracy                           0.72       359\n",
      "   macro avg       0.72      0.72      0.72       359\n",
      "weighted avg       0.72      0.72      0.72       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#From class \"Travis Martin\"\n",
    "X = test_data[\"text\"]\n",
    "X = tf.transform(X)\n",
    "y_true = test_data[\"polarity\"]\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(tfidf_matrix, train_data[\"polarity\"])\n",
    "predictions = neigh.predict(X)\n",
    "confusion_matrix(y_true,predictions)\n",
    "print(classification_report(y_true,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss your results here.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) AdaBoost\n",
    "\n",
    "In this exercise, you'll be learning how to use [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), as well as vizualize decision boundries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Using AdaBoost (Follow/Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris dataset and translate to pandas dataframe\n",
    "bc_sk = datasets.load_breast_cancer()\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# The fraction of data that will be test data\n",
    "test_data_fraction = 0.10\n",
    "\n",
    "bc_features = bc_data.iloc[:,0:-1]\n",
    "bc_labels = bc_data[\"target\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a baseline non-boosted decision tree to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed).fit(X=X_train, y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predicted_y = gini_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predicted_y,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(predicted_y,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get boosting. The default estimator for AdaBoost is a *decision stump*. (Remember: a decision stump is simply a decision tree with a height of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Adaboost classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the boosted model performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens when we put a more complex model into AdaBoost? Let's say, for this example, each component model is a full decision tree with no size limits.\n",
    "\n",
    "What do you expect will happen? Why? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try training a boosted model with full decision trees (no pruning)\n",
    "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed)\n",
    "bdt = AdaBoostClassifier(base_estimator=gini_tree,n_estimators=20,random_state=random_seed)\n",
    "bdt.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = bdt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predicted_y,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(predicted_y,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were the results what you expected? If they weren't, why not? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Visualizing Decision Boundries (Group)\n",
    "\n",
    "Now, we're going to peek under the hood and see the decision boundries of ensemble learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A nice noisy dataset that's not linearly separable\n",
    "X,y = make_moons(300, noise=0.13,random_state = random_seed)\n",
    "colors = {0:'red',1:\"blue\"}\n",
    "plt.scatter(X[:,0],X[:,1],c=np.vectorize(colors.get)(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're given some code to calculate Adaboost by hand below, but for it to work, you need to complete the following helper functions.\n",
    "\n",
    "In `calculate_alpha`, you should calculate alpha for the round, based on the classifier's predictions and the weights of that round. The basic steps are:\n",
    "1. Calculate the **weighted** error $\\epsilon$ for the round by comparing those predictions to y. Remember, the error is the sum of the weights of misclassified instances.\n",
    "2. Calculate alpha using the following formula:\n",
    "\n",
    "$$\\alpha = 0.5*ln(\\frac{1 - \\epsilon}{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alpha(predictions, weights, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        predictions: the predictions (0 or 1) of the classifier in this round.\n",
    "        weights: the weights of each instance at the end of last round\n",
    "        y: the class labels (0 or 1) for the instances\n",
    "    Output: the alpha value for this round\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test calculate_alpha, we need to make a decision tree and some starting weights\n",
    "dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=1, random_state=123).fit(X, y)\n",
    "predictions = dt.predict(X)\n",
    "weights = np.repeat(1 / len(X), len(X))\n",
    "# Alpha should be ~0.84\n",
    "alpha = calculate_alpha(predictions, weights, y)\n",
    "np.testing.assert_almost_equal(alpha, 0.8416209435087314)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to write a function that will update the weights for the adaboost classifier based on the alpha value. Remember:\n",
    "* Correctly classified instances have their weight decreased.\n",
    "* Incorrectly classified instances have their weight increased.\n",
    "\n",
    "The amount by which the weight changes is $e^\\alpha$.\n",
    "\n",
    "**Note**: You do not need to normalize the weights to sum to 1 - the code below will do that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def update_weights(predictions, weights, y, alpha):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        predictions: the predictions (0 or 1) of the classifier in this round.\n",
    "        weights: the weights at the end of the previous round for each instance\n",
    "        y: the class labels (0 or 1) for the instances\n",
    "        alpha: the alpha value for this round\n",
    "    Output: an array of updated weights for this round\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.repeat(1 / len(X), len(X))\n",
    "new_weights = update_weights(predictions, weights, y, alpha) \n",
    "alpha = 0.8416209435087314\n",
    "print(new_weights)\n",
    "np.testing.assert_almost_equal(max(new_weights), 1/300*math.exp(alpha))\n",
    "np.testing.assert_almost_equal(min(new_weights), 1/300/math.exp(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our code for Adaboost. You don't have to modify it, just finish the functions above.\n",
    "\n",
    "Why are we implementing it by hand? This way we can keep track of which samples were chosen each round to visualize them.\n",
    "\n",
    "When you run the following code, you can see the alpha values for each round, changing as the weights, and therefore the samples, change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's do 20 rounds of adaboost\n",
    "rounds = 20\n",
    "# N is the length of training data\n",
    "N = len(X)\n",
    "# Each boostrapped sample will b size N/5\n",
    "sample_size = N // 5\n",
    "\n",
    "# Each round, keep track of:\n",
    "# Which instances (indices) were samples\n",
    "sample_indices = []\n",
    "# The classifier built on that sample\n",
    "dtrees = []\n",
    "# The alpha value for each that round\n",
    "alphas = []\n",
    "\n",
    "# Start our weights off each as 1/N\n",
    "weights = np.repeat(1 / N, N)\n",
    "\n",
    "for i in range(rounds):\n",
    "    indices = np.random.choice(range(N), sample_size, replace=True, p=weights)\n",
    "    sample_X = X[indices,:]\n",
    "    sample_y = y[indices]\n",
    "    # Train a simple decision tree on the sample\n",
    "    dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=1).fit(sample_X, sample_y)\n",
    "    predictions = dt.predict(X)\n",
    "    \n",
    "    # Calculate alpha\n",
    "    alpha = calculate_alpha(predictions, weights, y)\n",
    "    print(f'Round {i} Alpha: {alpha}')\n",
    "    \n",
    "    # Update weights\n",
    "    weights = update_weights(predictions, weights, y, alpha)\n",
    "    weights /= sum(weights)\n",
    "    \n",
    "    sample_indices.append(indices)\n",
    "    alphas.append(alpha)\n",
    "    dtrees.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes predictions for a given list of X features by taking the weighted sum of\n",
    "# its constituent classifiers' predictions.\n",
    "def boosting_predict(X_test):\n",
    "    return np.mean([(dtrees[i].predict(X_test) - 0.5) * alphas[i] for i in range(len(dtrees))], axis=0) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the image above, how should points (0, 0) and (1, 1) be classified?\n",
    "# Note that the output is continuous: > 0.5 means more likely to be 1.\n",
    "boosting_predict([[0, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot the predictions from individual rounds of adaboost. Note:\n",
    "* The background color indicates the prediction.\n",
    "* The faded dots were *not* sampled in this round (e.g. because of their low weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a provided method that will plot the decision boundries of each stump\n",
    "\n",
    "def plot_predictions(pred_function, sample_indices, subplot=plt, continuous=False):\n",
    "    plot_colors = \"br\"\n",
    "    plot_step = 0.02\n",
    "    class_names = \"AB\"\n",
    "\n",
    "    #\n",
    "\n",
    "    # Plot the decision boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = pred_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    if not continuous:\n",
    "        Z = np.round(Z)\n",
    "        cs = subplot.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    else:\n",
    "        cs = subplot.contourf(xx, yy, Z)\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        in_sample_idx = np.intersect1d(idx, sample_indices)\n",
    "        subplot.scatter(X[in_sample_idx, 0], X[in_sample_idx, 1],\n",
    "                    c=c, cmap=plt.cm.Paired,\n",
    "                    s=20, edgecolor='k',\n",
    "                    label=\"Class %s (Sampled)\" % n)\n",
    "        out_sample_idx = np.setdiff1d(idx, in_sample_idx)\n",
    "        subplot.scatter(X[out_sample_idx, 0], X[out_sample_idx, 1],\n",
    "                    c=c, cmap=plt.cm.Paired,\n",
    "                    s=20, edgecolor='k', alpha=0.1,\n",
    "                    label=\"Class %s (Not Sampled)\" % n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the first 12 rounds of adaboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 3\n",
    "\n",
    "figure, axis = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "figure.tight_layout()\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        idx = i*rows + j\n",
    "        subplot = plot_predictions(dtrees[idx].predict, sample_indices[idx], axis[i,j])\n",
    "        axis[i,j].title.set_text(f'Round {idx}: Alpha = {alphas[idx]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer the following questions:\n",
    "1. Which rounds are most important to the classification? What do they have in common?\n",
    "2. Do you notice any changes in the samples (and resulting classifiers) in later rounds of Adaboost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the whole adaboost classifier. Note the non-linear decision boundary.\n",
    "* The first plot shows the discrete boundary, comprised of all of the above weak classifiers.\n",
    "* The second plot shows the same prediction as a continuous boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(boosting_predict, range(len(X)), plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(boosting_predict, range(len(X)), plt, continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer the following questions:\n",
    "1. How can Adaboost create a non-linear boundary to accurately classify this shape?\n",
    "2. Which areas of the plot is the classifier most certain about, or least certain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
