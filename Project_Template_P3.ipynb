{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Template: Phase 3\n",
    "\n",
    "Below are some concrete steps that you can take while doing your analysis for phase 3. This guide isn't \"one size fit all\" so you will probably not do everything listed. But it still serves as a good \"pipeline\" for how to do data analysis.\n",
    "\n",
    "If you do engage in a step, you should clearly mention it in the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0) Evaluation\n",
    "\n",
    "Now that you have selected your models and have trained/tuned them, it's time to see how they stack up. Some important questsion to ask:\n",
    "\n",
    "1. How did your models compare to each other\n",
    "2. In what metrics do they differ, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Comparing Models\n",
    "\n",
    "To compare your models, you can try things such as:\n",
    "\n",
    "1. Doing multiple random restarts of training/test splits (code below)\n",
    "2. Using cross-validation\n",
    "\n",
    "In your report, report back the following metrics:\n",
    "\n",
    "**Classification**\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1\n",
    "* AUC\n",
    "\n",
    "**Regression**\n",
    "* MSE\n",
    "* MAE\n",
    "* $R^2$\n",
    "\n",
    "**Sample Evaluation Code**: Here is some sample code for the evaluation procedure. **You do not need to use the sample code if you feel that it wouldn't work with your pipeline, but you can use it as inspiration**. It runs a set number of trials using different splits, and returns back a dataframe, where each row represents a single random evaluation. It has 3 columns.\n",
    "    \n",
    "Model: The name of the model being evaluated\n",
    "\n",
    "Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "\n",
    "Score: The score of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\"\"\"\n",
    "Evaluates a classification model\n",
    "\"\"\"\n",
    "def evaluate_classification(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    \n",
    "    acc = accuracy_score(y_test_ev,predictions)\n",
    "    \n",
    "    classification = classification_report(y_true, y_pred)\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "Evaluates regression using MAE,MSE, and R^2\n",
    "\"\"\"\n",
    "def evaluate_regression(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    mae = mean_absolute_error(y_test_ev,predictions)\n",
    "    mse = mean_squared_error(y_test_ev,predictions)\n",
    "    r2 = r2_score(y_test_ev,predictions)\n",
    "    return {\"mae\":mae,\"mse\":mse,\"r2\":r2}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains and evaluates a single model on a random train/test split\n",
    "\"\"\"\n",
    "def evaluate_random(model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Switch this out with `evaluate_regression` if you're doing a regression problem\n",
    "    evals = evaluate_classification(model,X_test,y_test)\n",
    "    #evals = evaluate_regression(model,X_test,y_test)\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: Your features\n",
    "    y: Your target\n",
    "    models: A list of the models that you are evaluating\n",
    "    n_trials (opt): The number of random trials\n",
    "    \n",
    "Output:\n",
    "    A dataframe with three colums and len(models)*n_trials*(number of evaluation metrics) rows.\n",
    "    Each row represents a single random evaluation.\n",
    "    \n",
    "    Model: The name of the model being evaluated\n",
    "    Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "    Score: The score of the evaluation\n",
    "\"\"\"\n",
    "def get_scores(X,y,models,n_trials=5):\n",
    "    \n",
    "    data = {\n",
    "        \"model\": [],\n",
    "        \"evaluation\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    \n",
    "    for n in range(n_trials):\n",
    "        for model in models:\n",
    "            # Put in special sampling methods\n",
    "            \n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "            # Put in feature scaling here\n",
    "            # MinMaxScaler()\n",
    "            \n",
    "            scores = evaluate_random(model,X_train,y_train,X_test,y_test)\n",
    "            \n",
    "            for key in scores:\n",
    "                data[\"model\"].append(str(model))\n",
    "                data[\"evaluation\"].append(key)\n",
    "                data[\"score\"].append(scores[key])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of getting classification scores\n",
    "# (See \"Follow\" doc for how to do it with regression)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = load_breast_cancer(return_X_y=True,as_frame=True)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "get_scores(X,y,[neigh],5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a boxplot of the different random runs (see Follow document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table with the average evaluation scores of each metric for each model\n",
    "**Bold** the best result for each.\n",
    "\n",
    "Here's an example for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | LinReg | SVR   | MLP   |\n",
    "|-----|--------|-------|-------|\n",
    "| **MAE** | 3.061  | 4.143 | **2.71**  |\n",
    "| **MSE** | 22.09  | 42.42 | **15.37** |\n",
    "| **$R^2$** | 0.684  | 0.394 | **0.780** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0) Technical Retrospective\n",
    "\n",
    "Now that you have your final model, go back and look at how your decisions impacted the results. This can take many forms, here are some ideas:\n",
    "\n",
    "* Which of your decisions were helpful? With your best model:\n",
    "    * Compare the results of the model with an without your feature selection\n",
    "    * Compare the results with and without feature engineering\n",
    "    * Compare if your sampling method made a difference\n",
    "    \n",
    "    \n",
    "* Why did your model do well?\n",
    "    * If your model is interpretable, discuss feature importance (e.g. decision tree splits, coefficients of linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0) Writeup \n",
    "\n",
    "Now it is time to reflect upon your work and tie up your report. The goal of this project was to get you familiar with doing a data science problem from scratch on a custom dataset. First, write some conclusions about your model. Then, consider how it could be used in practice. Finally, write about your experiences and what you learned from this project.\n",
    "\n",
    "Use the following questions as inspiration.\n",
    "\n",
    "1. How could we use this model in practice?\n",
    "2. Would you trust the model to make decisions?\n",
    "3. What are the limitations of the model?\n",
    "4. what are alternative approaches you could try in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
