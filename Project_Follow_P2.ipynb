{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Phase 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimporting\n",
    "\n",
    "First, let's reimport all of the data we finalized during our EDA session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./project_data/USVideos.csv\")\n",
    "\n",
    "with open(\"./project_data/US_category_id.json\") as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "cat_map = {}\n",
    "for index,cat in enumerate(categories[\"items\"]):\n",
    "    cat_map[int(cat[\"id\"])]=cat[\"snippet\"][\"title\"]\n",
    "    \n",
    "df[\"category\"] = df[\"category_id\"].map(cat_map)\n",
    "\n",
    "data = df[[\"title\",\"channel_title\",\"category\",\"description\",\"tags\",\"views\"]]\n",
    "\n",
    "data[\"log_views\"] = np.log(data[\"views\"]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take stock of what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>log_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>13.525659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>14.698775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>14.975981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>12.745978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>I know it's been a while since we did this sho...</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>14.555413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "         category                                        description  \\\n",
       "0  People & Blogs  SHANTELL'S CHANNEL - https://www.youtube.com/s...   \n",
       "1   Entertainment  One year after the presidential election, John...   \n",
       "2          Comedy  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...   \n",
       "3   Entertainment  Today we find out if Link is a Nickelback amat...   \n",
       "4   Entertainment  I know it's been a while since we did this sho...   \n",
       "\n",
       "                                                tags    views  log_views  \n",
       "0                                    SHANtell martin   748374  13.525659  \n",
       "1  last week tonight trump presidency|\"last week ...  2418783  14.698775  \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  14.975981  \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168  12.745978  \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  14.555413  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split our dataset into features and class variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"views\",axis=1)\n",
    "Y = data[\"log_views\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESPN                                      203\n",
       "The Tonight Show Starring Jimmy Fallon    197\n",
       "Vox                                       193\n",
       "TheEllenShow                              193\n",
       "Netflix                                   193\n",
       "Name: channel_title, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see if they're together\n",
    "X[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Late Show with Stephen Colbert        155\n",
       "ESPN                                      149\n",
       "Late Night with Seth Meyers               143\n",
       "The Tonight Show Starring Jimmy Fallon    142\n",
       "TheEllenShow                              142\n",
       "Name: channel_title, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuzzFeedVideo                             59\n",
       "The Tonight Show Starring Jimmy Fallon    55\n",
       "WIRED                                     54\n",
       "Netflix                                   54\n",
       "ESPN                                      54\n",
       "Name: channel_title, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_raw[\"channel_title\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your training data to fit any transformers or encoder your need, then apply the fit transformer to your test data. This applies to:\n",
    "* Normalizing/standardizing your features\n",
    "* Using Bag of Words or TF-IDF to encode strings\n",
    "* PCA or dimensionality reduction\n",
    "\n",
    "**Rationale**: In practice, we won't be able to see the test data we'll be making predicting for, so we shouldn't use that data as the basis for any transformation or feature extraction.\n",
    "\n",
    "Here, we **only take the top N** words when doing our count vectors. We do this because to use the full word list would be infeasable since there are so many. Also, due to the nature of language, there are probably only a few words that are used frequently - but many words used only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies our feature transformation to our training and test data\n",
    "# top_n\n",
    "\n",
    "def apply_feature_transformation(X_train, X_test,top_n=100):\n",
    "    # First, create and fit our vectorizers on the training data\n",
    "    # We only take the top_n words into consideration, and simply ignore anything outside of that\n",
    "    tag_vectorizer = CountVectorizer(token_pattern=r\"([^|]+)\", max_features=top_n)\n",
    "    tag_vectors = tag_vectorizer.fit_transform(X_train[\"tags\"].str.replace(\"\\\"\",\"\")).toarray()\n",
    "\n",
    "    description_vectorizer = CountVectorizer(max_features=top_n)\n",
    "    description_vectors = description_vectorizer.fit_transform(X_train[\"description\"].values.astype('U')).toarray()\n",
    "\n",
    "    title_vectorizer = CountVectorizer(max_features=top_n)\n",
    "    title_vectors = title_vectorizer.fit_transform(X_train[\"title\"]).toarray()\n",
    "\n",
    "    # Next, we concatinate the tag, description, and title vectors into one vector\n",
    "    # This is so we can have one \"object\" that we can fit into our models\n",
    "    # Therefore, this will create a matrix of size [num_train_instances * (3 * top_n)]\n",
    "    X_train_transfomed = np.concatenate((tag_vectors, description_vectors, title_vectors), axis=1)\n",
    "    \n",
    "    # Now we fit our test data\n",
    "    # This will create a matrix of size [num_test_instances * (3 * top_n)]\n",
    "    X_test_transformed = np.concatenate(\n",
    "        (\n",
    "            tag_vectorizer.transform(X_test[\"tags\"].str.replace(\"\\\"\",\"\")).toarray(),\n",
    "            description_vectorizer.transform(X_test[\"description\"].values.astype('U')).toarray(),\n",
    "            title_vectorizer.transform(X_test[\"title\"]).toarray()\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return X_train_transfomed, X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = apply_feature_transformation(X_train_raw,X_test_raw,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30711, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to make sure we shaped our features right\n",
    "# There should be 3*top_n features\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10238, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30711, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10238, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Models\n",
    "\n",
    "Now that we have our data fully preprocessed, let's try it with some different models to see what we can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "It's always good to work your way up from the simplest models you know before trying the more complicated ones. So let's see if a simple linear regression will solve our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score 0.2489374603044553\n",
      "Testing score: 0.2334513260988368\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score\", reg.score(X_train,y_train))\n",
    "print(\"Testing score:\", reg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eugh. Looks like linear regression isn't powerful enough for this problem. There's no use trying to tune this model if even the base accuracy doesn't look promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the KNN Regression takes some time, so we'll only evaluate the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.8336338911371903\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing score:\", neigh.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Regression\n",
    "\n",
    "Let's see if we can squeeze anything out of a neural network. (What you didn't see was fiddling with the params and layer topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(300, 100, 50, 10), max_iter=45)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = MLPRegressor(max_iter=45, hidden_layer_sizes = (300,100,50,10))\n",
    "nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236261434884704"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8703367358279579"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To continue, let's choose the KNN model**. This model is semi-interpretable, so it's perfered over the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "For the sake of time, we'll only be finding hyperparams for our KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input: The training X features and Y labels/values\n",
    "    Output: The classifier with the best hyperparams, the predictions\n",
    "    \"\"\"\n",
    "    neigh = KNeighborsRegressor()\n",
    "    param_grid = {\"n_neighbors\": [1,2],\n",
    "                  \"p\": [1,2]}\n",
    "    \n",
    "    # Warning, takes a while!\n",
    "    search = GridSearchCV(neigh, param_grid)\n",
    "    search.fit(X_train,y_train)\n",
    "    return search, search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, predictions = find_best_hyperparameters(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsRegressor(),\n",
       "             param_grid={'n_neighbors': [1, 2], 'p': [1, 2]})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.26273374, 14.49446214, 14.19448261, ..., 10.08341666,\n",
       "       16.12511238, 12.28874602])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
